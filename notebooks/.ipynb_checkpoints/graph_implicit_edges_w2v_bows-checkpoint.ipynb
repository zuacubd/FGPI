{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the data path ...\n",
      "Approach ..\n",
      "Loading word2vec models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 937452.70it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1031872.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loading the tweetID and messages ...\n",
      "Total tweets: 17652\n",
      "Loading the retweerUser, tweetUser, and tweetId ...\n",
      "Total retweets: 20000\n",
      "Loading the lexicon file ...\n",
      "Total jargons in lexicon: 189\n",
      "Extracting tweetUser and tweetId ...\n",
      "Total users who tweeted: 15372\n",
      "Extracting retweetUser and tweetId ...\n",
      "Total users who retweeted: 19269\n",
      "Getting both tweet and retweet users ...\n",
      "Total users who both tweeted and retweeted: 514\n",
      "Getting all users either tweeter or retweeter ...\n",
      "Total user who tweeted or retweeted: 34127\n",
      "Ploting the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Estimating users lexicon representation ...\n",
      "done.\n",
      "Combining users lexicon and word2vec representation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/sig/mullah/.conda/envs/e36t11/lib/python3.6/site-packages/ipykernel_launcher.py:308: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 848508.34it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 983389.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Loading explicit links ...\n",
      "Total explicit links: 20000\n",
      "Total nodes with degree at most 1 is 31169\n",
      "Removing nodes ....\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the candidate users and its representation as data frame\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34127 entries, 0 to 34126\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 78.1 MB\n",
      "None\n",
      "Computing dot product divide by the length...\n",
      "189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3277/34127 [00:00<00:00, 32769.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users x Users similarity dim: 34127,34127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:01<00:00, 32806.90it/s]\n",
      "  0%|          | 35/34127 [00:00<01:37, 349.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_sim:-0.004903849933749784, Max_sim:0.10662238926271952\n",
      "User:3171, Similarities: [0. 0. 0. ... 0. 0. 0.]\n",
      "Assessing the candidate users with edge weight 0.010662238926271952 for step 1\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:54<00:00, 629.75it/s] \n",
      "  0%|          | 36/34127 [00:00<01:36, 353.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 2097731 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.021324477852543905 for step 2\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:52<00:00, 647.90it/s] \n",
      "  0%|          | 37/34127 [00:00<01:33, 365.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 1894618 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.03198671677881586 for step 3\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:51<00:00, 656.78it/s] \n",
      "  0%|          | 42/34127 [00:00<01:22, 411.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 1762003 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.04264895570508781 for step 4\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 830.88it/s] \n",
      "  0%|          | 42/34127 [00:00<01:22, 414.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 3003 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.05331119463135976 for step 5\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 827.11it/s] \n",
      "  0%|          | 42/34127 [00:00<01:21, 417.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 526 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.06397343355763171 for step 6\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 815.66it/s] \n",
      "  0%|          | 42/34127 [00:00<01:22, 414.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 96 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.07463567248390367 for step 7\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 821.67it/s] \n",
      "  0%|          | 42/34127 [00:00<01:21, 416.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 93 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.08529791141017562 for step 8\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 824.47it/s] \n",
      "  0%|          | 43/34127 [00:00<01:20, 421.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 15 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.09596015033644757 for step 9\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:40<00:00, 841.08it/s] \n",
      "  0%|          | 43/34127 [00:00<01:20, 421.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 15 and maximum weight: 0.107\n",
      "Assessing the candidate users with edge weight 0.10662238926271952 for step 10\n",
      "Generating implicit links ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34127/34127 [00:41<00:00, 817.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total implicit links: 15 and maximum weight: 0.107\n",
      "done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEZCAYAAAC99aPhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnc0lEQVR4nO3de7wVdb3/8ddbUMQLeAFvgGKKFVqS7tSyjpb+lOyidTTxZKJxIv3pz0vaOdqxslOW1k89WT81+qmo5YW0UitNj/fStI2iiEShEiKKmBfQDAU/54/vd8ewXHvt2Xv2bcH7+XjMY836znw/851Zl8+a+c6aUURgZmbWVWv1dQPMzKy5OZGYmVklTiRmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJP2MpIskfaWbYm0t6RVJA/LzOyX9a3fEzvFukjSxu+J1YrnflPS8pGd7e9nWP0k6UtJv25k2WlJIGtjb7VpTOJH0IknzJL0maamklyTdK+loSf94HSLi6Ij4RslY+zaaJyLmR8QGEbGiG9p+hqQf18T/SERcVjV2J9sxCjgZGBsRW9SZvrekBb3Zprzcuq+HpH+RdGUXY4ak7au3rlPLnCrpm725zLzcbv2RY73LiaT3fTwiNgS2Ac4C/h24uLsXshr/+toG+GtEPNfXDSnpAODXtYWr8etjFTXleyMiPPTSAMwD9q0p2w14E9gpP58KfDOPDwN+CbwEvADcQ0r+V+Q6rwGvAP8GjAYCmATMB+4ulA3M8e4Evg08ALwMXA9skqftDSyo115gPPA68EZe3sOFeP+ax9cCTgf+AjwHXA4MzdPa2jExt+154D8abKehuf7iHO/0HH/fvM5v5nZMram3fs30V4CtctmwPM/pwHJgSH7+TeC/8vgg4P/mNi4CLgIGF+J/DJiRX497gXfn8re8HoVtsii/jm95ffI8nwNmAy8CvwG2yeV35/lfzTEPBe4C/jlP/0CefkB+vi8wo9DWunHztHcAt5LeU3OAT+fyyfk1fj0v88Z2Xp8AjgeeyK/ld4G1SrwP1gV+DPw1b8M/AJsDZwIrgL/n5f6gUTvztE2BG4AlpPfzN4DfttPetm0/GVgIPAOcnKdtAfwN2LQw/66k997adWJNJX8+631uSD8MnwaW5jbvU9gupwKP5/WfxsrP3lveG+1tq77+Dmv3M9vXDViTBuokklw+Hzgmj//jjUr60r8IWDsPHwRUL1bhzXg56Qt1MPUTydPATnme64Af52mrfCBqlwGc0TZvYfqdrEwknwPmAm8DNgB+BlxR07Yf5XbtDCwD3tnOdrqclOQ2zHX/BExqr501deutx92s/AK+JX+YP1KY9sk8/l+kL6dN8rJvBL6dp+1C+mLcHRhASorzgEHtvbbAHsB9DV6fg/I2eycwkPQFfG+hfgDbF57/J/D9PP7lvB5nF6Z9L4+3Gzcv+yngqDxtF1Iy2LH2/ddgGwdwR95OW+fXp8z74At5m66Xt+GurEzod7bFKNnOq0lfxuuT3s9P03EiuSrP/y5Somh7b/+a/PnLz89r2851Yq2yfSi834C35zZvVVjudnn8ROD3wEjSD5YfAlc1eG+0u6364+BDW/3DQtKHstYbwJakX5NvRMQ9kd95DZwREa9GxGvtTL8iIh6NiFeBrwCfbuuMr+gzwLkR8UREvAKcBkyo2U3/ekS8FhEPAw+TEsoqclsOBU6LiKURMQ84B/hshbbdBeyV2/Ju4Pz8fF3gvcA9kgR8HjgpIl6IiKXAt4AJOcbngR9GxP0RsSJS39AyUrJoz0d562Gt4uvzBVKimh0Ry/PyxknaptF65PF/Iv3QaHu+V55OB3E/BsyLiEsjYnlEPEj6QXFwg/Wo5+y8neaTEvBhubzR++AN0p7E9nkbTo+IJe3Eb7ed+T3yz8BX87Z8FCjTV/f1PP9M4NJCmy8DDod/vP8OI+1ldtYKUpIYK2ntiJgXEY/naV8g7YUviIhlpB9mB9d8Porvjc5sqz7nRNI/jCDtvtf6LunX3S2SnpB0aolYT3Vi+l9IezrDSrWysa1yvGLsgaRDF22KZ1n9jfSLtdYwYJ06sUZUaNtdpF+OuwAzSYdL9iIlgbkR8TwwnPTrb3o+EeIl4OZcDqlv5uS2aXn6KNJ6t6de/0hx+28DfK8Q7wVAtL+u9wE7SNocGEf6BTtK0jDSIdK7S8TdBti9Zj0+QzrE0xm176O27dDofXAF6TDb1ZIWSvqOpLXbid+oncNzzNo2dLXN15O+/N8G/C/g5Yh4oES8VUTEXNKexxnAc5KultS2jG2AnxfWZTYp8RQ/H8X2dWZb9Tknkj4m6b2kD/hbTl3Mv8hPjoi3AR8Hvihpn7bJ7YTsaI9lVGF8a9Ivn+dJx+LXK7RrACu/RMvEXUj6sBRjLyf1EXTG87lNtbGeLlm/XjvvJR12+CRwV0Q8lmN+lJW/4p8n9XHsGBEb5WFoRLQlu6eAMwvTNoqI9SLiqnrLlbQFaW/ywQbtewr4Qk3MwRFxb90Vi/gbMB04AXg0Il7P6/ZF4PGcEDuK+1TeBsVpG0TEMQ22Xz2176OFebzd90Heq/56RIwF3k/a6ziineU2aufiHLO2DV1qc0T8nXSY7DOkPd9GeyOrfE6oScARcWVEfIC0DQI4u7A+H6lZn3Ujovi+jkKcRtuq33Ei6SOShkj6GOlY74/z7nbtPB+TtH0+7LKE9Aum7VTeRaTj0J11uKSxktYjHVe/NtLpwX8C1pX00fzL53TSbnqbRcDo4qnKNa4CTpK0raQNSIdTrsmHVkrLbZkGnClpw3w45oukjscyFgGbShpaiNn2BXwsKxPHvaTDDXfled4k9eGcJ2kzAEkjJO2f5/8RcLSk3ZWsn7fVhoXlFl+PA4CbOzgUeRFwmqQd8/KGSjqkZl1qX+O7gOMK63FnzfOO4v6StFfzWUlr5+G9kt7ZYJn1fEnSxvl07BOAa3J5u+8DSR+S9K78I2UJ6QdDe+/ndtuZ3yM/A86QtJ6ksaQ+q458Jc+/I6nv5ZrCtMuBI4FP0Pi9NgM4QNIm+cfCiW0TJL1d0oclDSKdOPBaYf0uIr2nt8nzDpd0YHsL6WBb9T9lO1M8VB9IHbKvkc7oeJl0qOJYYEBhnqms7Gw/Kdd5FVgAfKUw34GkTvqXgFOo6VjP86xSxqpnbS0hdeYNK8x/JOmMludyzHms7JDclLTX9CLwYCFe8aytr5J+eS0mfRg3rteO2rp1ttPGuf7iHO+rrDwraG8adLbneS5h5dkubR2f387bvq1z/Ljcps0L9dYlffE9kbfPbOD4wvTxpLNnXsrb6afAhu28HtcCB7f3WhTKP0s63LYkr+slhWlH5+W8xMozq/bPcfbKz3fKzw/tRNy3A7/K2/evwO3AuDxtDCvPTPtFO9s3WHnW1l9JfVgDSrwPDiOdyfQqKXGcz8r35vtIP2ZeBM4v0c7hpGTTlbO2niWfWVcz359Je0GN3lvrkhLQEuAR0me0rbP93bktS0mHE3/JyvffWqQfRHPy9MeBbzX4fLS7rfrj0HYGkJl1k9yB+izpjJ2X+7o93U1SAGMi9QmsNiTdDlwZEf+/r9vSbJrvjy9m/d8mpL3H1S6JrK5yX+UupD1L6yT3kZh1s4h4LiIu7Ot2WDmSLgP+Gzgx0mnf1kk+tGVmZpV4j8TMzCpxIjEzs0rWuM72YcOGxejRo/u6GWZmTWX69OnPR8TwetPWuEQyevRoWltb+7oZZmZNRVK7l6HxoS0zM6vEicTMzCpxIjEzs0qcSMzMrBInEjMzq8SJxMzMKnEiMTOzSpxIzMyskjXuD4mVSF2r5wtjmtlqzHskZmZWiROJmZlV4kRiZmaVOJGYmVklTiRmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJGZmVokTiZmZVdJjiUTSupIekPSwpFmSvp7LN5F0q6Q/58eNC3VOkzRX0hxJ+xfKd5U0M087X0rXKpE0SNI1ufx+SaN7an3MzKy+ntwjWQZ8OCJ2BsYB4yXtAZwK3BYRY4Db8nMkjQUmADsC44ELJA3IsS4EJgNj8jA+l08CXoyI7YHzgLN7cH3MzKyOHkskkbySn66dhwAOBC7L5ZcBB+XxA4GrI2JZRDwJzAV2k7QlMCQi7ouIAC6vqdMW61pgn7a9FTMz6x092kciaYCkGcBzwK0RcT+weUQ8A5AfN8uzjwCeKlRfkMtG5PHa8lXqRMRy4GVg0zrtmCypVVLr4sWLu2ntzMwMejiRRMSKiBgHjCTtXezUYPZ6exLRoLxRndp2TImIlohoGT58eAetNjOzzuiVs7Yi4iXgTlLfxqJ8uIr8+FyebQEwqlBtJLAwl4+sU75KHUkDgaHACz2xDmZmVl9PnrU1XNJGeXwwsC/wR+AGYGKebSJwfR6/AZiQz8TaltSp/kA+/LVU0h65/+OImjptsQ4Gbs/9KGZm1kt68g6JWwKX5TOv1gKmRcQvJd0HTJM0CZgPHAIQEbMkTQMeA5YDx0bEihzrGGAqMBi4KQ8AFwNXSJpL2hOZ0IPrY2ZmdWhN+wHf0tISra2tXavsW+2a2RpK0vSIaKk3zf9sNzOzSpxIzMysEicSMzOrxInEzMwqcSIxM7NKnEjMzKwSJxIzM6vEicTMzCpxIjEzs0qcSMzMrBInEjMzq8SJxMzMKnEiMTOzSpxIzMysEicSMzOrxInEzMwqcSIxM7NKnEjMzKwSJxIzM6vEicTMzCpxIjEzs0qcSMzMrBInEjMzq6THEomkUZLukDRb0ixJJ+TyMyQ9LWlGHg4o1DlN0lxJcyTtXyjfVdLMPO18ScrlgyRdk8vvlzS6p9bHzMzq68k9kuXAyRHxTmAP4FhJY/O08yJiXB5+DZCnTQB2BMYDF0gakOe/EJgMjMnD+Fw+CXgxIrYHzgPO7sH1MTOzOnoskUTEMxHxYB5fCswGRjSociBwdUQsi4gngbnAbpK2BIZExH0REcDlwEGFOpfl8WuBfdr2VszMrHf0Sh9JPuT0HuD+XHScpEckXSJp41w2AniqUG1BLhuRx2vLV6kTEcuBl4FN6yx/sqRWSa2LFy/unpUyMzOgFxKJpA2A64ATI2IJ6TDVdsA44BngnLZZ61SPBuWN6qxaEDElIloiomX48OGdWwEzM2uoRxOJpLVJSeQnEfEzgIhYFBErIuJN4EfAbnn2BcCoQvWRwMJcPrJO+Sp1JA0EhgIv9MzamJlZPT151paAi4HZEXFuoXzLwmyfBB7N4zcAE/KZWNuSOtUfiIhngKWS9sgxjwCuL9SZmMcPBm7P/ShmZtZLBvZg7D2BzwIzJc3IZV8GDpM0jnQIah7wBYCImCVpGvAY6YyvYyNiRa53DDAVGAzclAdIieoKSXNJeyITenB9zMysDq1pP+BbWlqitbW1a5W7ekLYGraNzWz1I2l6RLTUm+Z/tpuZWSVOJGZmVokTiZmZVeJEYmZmlTiRmJlZJU4kZmZWiROJmZlV4kRiZmaVOJGYmVklTiRmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJGZmVokTiZmZVeJEYmZmlTiRmJlZJU4kZmZWSacSiaS1JA3pqcaYmVnz6TCRSLpS0hBJ6wOPAXMkfannm2ZmZs2gzB7J2IhYAhwE/BrYGvhsTzbKzMyaR5lEsraktUmJ5PqIeAOIHm2VmZk1jTKJ5CJgHrA+cLekbYAlPdkoMzNrHg0TiaS1gEURMSIiDoiIAOYDH+oosKRRku6QNFvSLEkn5PJNJN0q6c/5ceNCndMkzZU0R9L+hfJdJc3M086XpFw+SNI1ufx+SaO7thnMzKyrGiaSiHgTOK6mLCJieYnYy4GTI+KdwB7AsZLGAqcCt0XEGOC2/Jw8bQKwIzAeuEDSgBzrQmAyMCYP43P5JODFiNgeOA84u0S7zMysG5U5tHWrpFPyHsYmbUNHlSLimYh4MI8vBWYDI4ADgcvybJeR+l7I5VdHxLKIeBKYC+wmaUtgSETcl/eILq+p0xbrWmCftr0VMzPrHQNLzPO5/HhsoSyAt5VdSD7k9B7gfmDziHgGUrKRtFmebQTw+0K1BbnsjTxeW95W56kca7mkl4FNgedrlj+ZtEfD1ltvXbbZZmZWQoeJJCK2rbIASRsA1wEnRsSSBjsM9SZEg/JGdVYtiJgCTAFoaWnxGWdmZt2ozB8S15N0uqQp+fkYSR8rEzyfNnwd8JOI+FkuXpQPV5Efn8vlC4BRheojgYW5fGSd8lXqSBoIDAVeKNM2MzPrHmX6SC4FXgfen58vAL7ZUaXcV3ExMDsizi1MugGYmMcnAtcXyifkM7G2JXWqP5APgy2VtEeOeURNnbZYBwO3534UMzPrJWX6SLaLiEMlHQYQEa+V7NDek/QP+JmSZuSyLwNnAdMkTSKdSnxIjjtL0jTSZViWA8dGxIpc7xhgKjAYuCkPkBLVFZLmkvZEJpRol5mZdaMyieR1SYPJfQ+StgOWdVQpIn5L/T4MgH3aqXMmcGad8lZgpzrlfycnIjMz6xtlEsnXgJuBUZJ+QtrTOLInG2VmZs2jzFlbt0p6kPSnQgEnRMTzHVQzM7M1RJmztvYE/h4RvwI2Ar6cr7dlZmZW6qytC4G/SdoZ+BLwF9K/y83MzEolkuX5lNoDgfMj4nvAhj3bLDMzaxZlOtuXSjoNOBz4p3whxbV7tllmZtYsyuyRHEo63XdSRDxLur7Vd3u0VWZm1jTKnLX1LHBu4fl83EdiZmZZh4lE0lJWXghxHdJhrVciYmhPNszMzJpDmT2SVTrWJR0E7NZTDTIzs+ZSpo9kFRHxC+DD3d8UMzNrRmUObX2q8HQtoIU69/wwM7M1U5nTfz9eGF8OzCP9p8TMzKxUH8lRvdEQMzNrTp3uIzEzMytyIjEzs0raTSSSTsiPe/Zec8zMrNk02iNp6xv5fm80xMzMmlOjzvbZkuYBwyU9UigXEBHx7h5tmZmZNYV2E0lEHCZpC+A3wCd6r0lmZtZMGp7+my/YuLOkdYAdcvGciHijx1tmZmZNocw/2/ciXe13Humw1ihJEyPi7h5um5mZNYEy/2w/F9gvIuYASNoBuArYtScbZmZmzaHM/0jWbksiABHxJ0rcIVHSJZKek/RooewMSU9LmpGHAwrTTpM0V9IcSfsXyneVNDNPO1+ScvkgSdfk8vsljS65zmZm1o3KJJJWSRdL2jsPPwKml6g3FRhfp/y8iBiXh18DSBoLTAB2zHUuyLf0BbgQmAyMyUNbzEnAixGxPXAecHaJNpmZWTcrk0iOAWYBxwMnAI8BR3dUKfehvFCyHQcCV0fEsoh4EpgL7CZpS2BIRNwXEUHqqzmoUOeyPH4tsE/b3oqZmfWeMhdtXEbqJzm3o3lLOk7SEUArcHJEvEi6D/zvC/MsyGVv5PHacvLjU7mNyyW9DGwKPN9N7TQzsxJ6+1pbFwLbAeOAZ4Bzcnm9PYloUN6ozltImiypVVLr4sWLO9VgMzNrrFcTSUQsiogVEfEm8CNW3rJ3ATCqMOtIYGEuH1mnfJU6kgYCQ2nnUFpETImIlohoGT58eHetjpmZ0cuJJPd5tPkk0HZG1w3AhHwm1rakTvUHIuIZYKmkPXL/xxHA9YU6E/P4wcDtuR/FzMx6UZn/kbyFpMkRMaWDea4C9gaGSVoAfA3YW9I40iGoecAXACJilqRppI785cCxEbEihzqGdAbYYOCmPABcDFwhaS5pT2RCV9bFzMyq6VIioX7/xCoi4rA6xRc3mP9M4Mw65a3ATnXK/w4c0lE7zMysZ3Xp0FZE/LC7G2JmZs2pw0QiaaSkn0taLGmRpOskjeyonpmZrRnK7JFcSurY3pL0340bc5mZmVmpRDI8Ii6NiOV5mAr4HFozMwPKJZLnJR0uaUAeDgf+2tMNMzOz5lAmkXwO+DTwLOnf6AfnMjMzs1LX2pqPb7VrZmbtaDeRSPpqg3oREd/ogfaYmVmTabRH8mqdsvVJ9wHZFHAiMTOz9hNJRLRdmRdJG5LuRXIUcDUrr9prZmZruIZ9JJI2Ab4IfIZ0E6ld8v1DzMzMgMZ9JN8FPgVMAd4VEa/0WqvMzKxpNDr992RgK+B0YKGkJXlYKmlJ7zTPzMz6u0Z9JL1990QzM2tCThZmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJGZmVokTiZmZVeJEYmZmlTiRmJlZJU4kZmZWSY8lEkmXSHpO0qOFsk0k3Srpz/lx48K00yTNlTRH0v6F8l0lzczTzpekXD5I0jW5/H5Jo3tqXczMrH09uUcyFRhfU3YqcFtEjAFuy8+RNBaYAOyY61wgaUCucyEwGRiTh7aYk4AXI2J74Dzg7B5bEzMza1ePJZKIuBt4oab4QNLl6MmPBxXKr46IZRHxJDAX2E3SlsCQiLgvIgK4vKZOW6xrgX3a9lbMzKz39HYfyeYR8QxAftwsl48AnirMtyCXjcjjteWr1ImI5cDLpDs3mplZL+ovne319iSiQXmjOm8NLk2W1CqpdfHixV1sopmZ1dPbiWRRPlxFfnwuly8ARhXmGwkszOUj65SvUkfSQGAobz2UBkBETImIlohoGT58eDetipmZQe8nkhuAiXl8InB9oXxCPhNrW1Kn+gP58NdSSXvk/o8jauq0xToYuD33o5iZWS9qeM/2KiRdBewNDJO0APgacBYwTdIkYD5wCEBEzJI0DXgMWA4cGxErcqhjSGeADQZuygPAxcAVkuaS9kQm9NS6mJlZ+7Sm/YhvaWmJ1tbWrlXu6klha9g2NrPVj6TpEdFSb1p/6Ww3M7Mm5URiZmaVOJGYmVklTiRmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJGZmVokTiZmZVeJEYmZmlTiRmJlZJU4kZmZWiROJmZlV4kRiZmaVOJGYmVklTiRmZlaJE4mZmVXiRGJmZpU4kZiZWSVOJGZmVokTiZmZVeJEYmZmlTiRmJlZJX2SSCTNkzRT0gxJrblsE0m3Svpzfty4MP9pkuZKmiNp/0L5rjnOXEnnS1JfrI+Z2ZqsL/dIPhQR4yKiJT8/FbgtIsYAt+XnSBoLTAB2BMYDF0gakOtcCEwGxuRhfC+238zM6F+Htg4ELsvjlwEHFcqvjohlEfEkMBfYTdKWwJCIuC8iAri8UMfMzHpJXyWSAG6RNF3S5Fy2eUQ8A5AfN8vlI4CnCnUX5LIReby23MzMetHAPlrunhGxUNJmwK2S/thg3nr9HtGg/K0BUrKaDLD11lt3tq1mZtZAn+yRRMTC/Pgc8HNgN2BRPlxFfnwuz74AGFWoPhJYmMtH1imvt7wpEdESES3Dhw/vzlUxM1vj9XoikbS+pA3bxoH9gEeBG4CJebaJwPV5/AZggqRBkrYldao/kA9/LZW0Rz5b64hCHTMz6yV9cWhrc+Dn+UzdgcCVEXGzpD8A0yRNAuYDhwBExCxJ04DHgOXAsRGxIsc6BpgKDAZuyoOZmfUipROe1hwtLS3R2tratcpd/ZvKGraNzWz1I2l64e8aq+hPp/+amVkTciIxM7NKnEjMzKwSJxIzM6vEicTMzCpxIjEzs0qcSMzMrBInEjMzq8SJxMzMKnEiMTOzSpxIzMysEicSMzOrxInEzMwqcSIxM7NKnEjMzKwSJxIzM6vEicTMzCpxIjEzs0r64p7tazbfrtfMVjPeIzEzs0qcSMzMrBInEjMzq8SJxMzMKnEiMTOzSpo+kUgaL2mOpLmSTu3r9piZrWmaOpFIGgD8P+AjwFjgMElj+7ZVvUDq2mBm1gOaOpEAuwFzI+KJiHgduBo4sI/bZGa2Rmn2PySOAJ4qPF8A7F47k6TJwOT89BVJc3qgLcOA5+tOKb830P9jdE53xHEMx+jpGN0Zp6r+tE1qbdPehGZPJPW+Gd/yF/CImAJM6dGGSK0R0eIY/a8tjuEYvRWnP7SjL9al2Q9tLQBGFZ6PBBb2UVvMzNZIzZ5I/gCMkbStpHWACcANfdwmM7M1SlMf2oqI5ZKOA34DDAAuiYhZfdSc7jh0tjrF6K44juEYPR2jO+NU1Z+2SWkKX1XWzMwqaPZDW2Zm1secSMzMrBInEjMzq8SJpA9JeoekfSRtUFM+vhMxdpP03jw+VtIXJR1QsV2XV6z/gdyO/TpRZ3dJQ/L4YElfl3SjpLMlDe1EnOMljep4znbrryPpCEn75uf/IukHko6VtHYnY20n6RRJ35N0jqSjO7MuZs3Cne3dTNJREXFpifmOB44FZgPjgBMi4vo87cGI2KVEjK+RrjM2ELiV9K/+O4F9gd9ExJklYtSeLi3gQ8DtABHxiRIxHoiI3fL45/N6/RzYD7gxIs4qEWMWsHM+E28K8DfgWmCfXP6pjmLkOC8DrwKPA1cBP42IxWXq5vo/IW3P9YCXgA2An+V2KCImloxzPPBx4C7gAGAG8CLwSeB/R8SdZdtkjUnaLCKe6+t2rNEiwkM3DsD8kvPNBDbI46OBVlIyAXioEzEGkL70lgBDcvlg4JGSMR4EfgzsDeyVH5/J43uVjPFQYfwPwPA8vj4ws2SM2cU21Uyb0Ynt/xBpT3s/4GJgMXAzMBHYsET9R/LjQGARMCA/V9ltWnxt8vh6wJ15fOtOvL5DgbOAPwJ/zcPsXLZRN7xXbyo53xDg28AVwL/UTLugZIwtgAtJF1ndFDgjb6NpwJadaPMmNcOmwDxgY2CTkjHG12zji4FHgCuBzatu106sy4PA6cB2FWK0AHfkz/Ao0g/Kl/Pn8D29tS4+tNUFkh5pZ5gJbF4yzICIeAUgIuaRvsA/Iulc6l/6pZ7lEbEiIv4GPB4RS3K814A3S8ZoAaYD/wG8HOmX8msRcVdE3FUyxlqSNpa0KelX++LcjleB5SVjPCrpqDz+sKQWAEk7AG+UjJEXG29GxC0RMQnYCrgAGA88UXJd1gE2JCWAtkNRg4BOHdpi5f+0BuV4RMT8TsSZRtqL2TsiNo2ITUl7iy8CPy0TQNIu7Qy7kvaEy7iU9J68Dpgg6TpJg/K0PUrGmAo8Rro23h3Aa8BHgXuAi0rGgHQNqemFoZV0zb0H83gZ3yqMn0P64fRx0pfvDzvRlqo2BjYC7pD0gKSTJG3VyRgXAN8BfgXcC/wwIoYCp+ZpvaO3MtbqNJB+qY4jXcSsOIwGFpaMcTswrqZsIHA5sKJkjPuB9fL4WoXyodT8qi8RayTpy+kHlNyrKtSdR/qSfjI/bpHLN6Dk3kRu81TSIan7ScnjCdKhoZ070ZaHGkwbXKL+SXm5fwGOB24DfkT69fy1TrTjBNKv3CmkPYqjcvlw4O6SMeZ0ZVrNfCvye+2OOsNrJWPMqHn+H8DvSHsDpd5nrLrXOr9R/A7inELaw3xXoezJTr5fH2xv2Z1pS9Whph0fJH3xP5tfm8ndsF0f6mrbOr0uvbWg1Wkg7Qp/oJ1pV5aMMbLtC7fOtD1LxhjUTvmw4getk+v2UeBb3bSd1gO27WSdDYGdgV3pwmEGYIduaPdWwFZ5fCPgYGC3LsTZMdd9RxfbcQvwb8XtQNrj/Xfgv0vGeBQY0860p0rGmE3hh0oumwjMAv5SMsbDhfFv1kwrdfizMH/bj55z8/vliU7WXwB8ETiZ9KNBhWmlD192w/vsLUmYdKh6PHBpyRj3kQ7jHkL68XNQLt8LaO2tdXFnu1k/JWlj0iGKA4HNcvEi0vXkzoqIF0vEOJj0Rf2WWydIOigiflEixneAWyLiv2vKxwPfj4gxJWL8J/CdyIdzC+Xbk9bl4I5i1In5cdLe0eiI2KIT9b5WU3RBRCyWtEVu4xGdbUtXSLo6IiZUjLEz6dDWm6S96WNISf5p4PMRcW/lhpZphxOJWfMpe3bg6h5D0mBSZ/Wjfd2W7tRs6+JEYtaEJM2PiK0do3+2papmW5emvvqv2epM0iPtTaLk2YGrU4z+1paqVqd1cSIx6782B/Ynne5bJNKpnmtajP7WlqpWm3VxIjHrv35J+tPqjNoJku5cA2P0t7ZUtdqsi/tIzMysEv+z3czMKnEiMTOzSpxIrKlICknnFJ6fIumMHljOVfn6aSfVlB8kaWx3L68Qf1zZ2wBIulnSiE7EPlHSel1vXYfxe3TbWP/lRGLNZhnwKUnDemoB+R/O74+Id0fEeTWTDwJ68styHOmy8w3lP+JtEhFP15QPaFDtRNJla3rKQfTstrF+yonEms1y0oUQT6qdIGkbSbflPYnbJDX8M5akdSVdKmmmpIckfShPugXYTNIMSR8szP9+4BPAd/O03SVNz9N2zntLW+fnj0taT9LwfLXcP+Rhzzx9fUmX5LKHJB2Yrzr8n8ChOf6hkvbK4zPyfBvm5uxNuvcMkuZJ+qqk3wKHSNpP0n2SHpT0U0kbKN0fZSvSlWbvkPRppStNI+kESU/k8e1yHCTtKukuSdMl/UbSloV5bs7l9yjdoK1222yndJOxx/LrcXUnXmNrNr11US8PHrpjAF4h3R9jHumKwacAZ+RpNwIT8/jngF90EOtk8sXxgHcA84F1SVdxfrSdOlOBgwvPZ+X2HEe6DPlnSFeCvi9Pv5J8gU/SvUhm5/FvAYfn8Y2AP5Hu33Ik8INC/BvJF/EkXU15YB4/H/hwHp8H/FseHwbcDayfn/878NXCfMPy+BbAH/L4tbntI0jXafo26VL397Ly3jKHApfk8dvIF4Ik3Uzt9na2zULyhUXphvuneOi/g/9HYk0nIpYo3Q74eNJ9Ldq8D2i7k+IVpIvZNfIB4Ps55h8l/QXYgXSTsLLuBfYE/omUHMaT/gx2T56+LzBW+sctZobkvYr9gE9IOiWXr0tKNLV+B5yrdOfGn0XEgly+JymJtrkmP+5BOrz0u7zMdUhXiF1FRDyb91Q2JN0Q6cq8Dh8k3RHy7cBOwK05zgDgGaXbQr8f+GlhnQZR3yPATyT9AvhFO/PYasCJxJrVf5FuZtToonQd/Umq7A3EGrmH9OW7DXA9aQ8gSH8Ug3T4+H2Rbja2csHpW/ifo+aqvJJ2Lz6PiLMk/YrUb/J7pXvJv066BPzrhVlfLazTrRFxWIm23wccBczJ6/E5UjI+mZTUZkXE+2raNwR4KSLGlYj/UVJy+gTwFUk7RkTZG51ZE3EfiTWliHiBdAfBSYXie4G2y3J/BvhtB2HuzvO13Ylxa9KXaiNLyXc7LMQ4HPhzRLwJvED60v9dnn4L6bAXeTnj8uhvgP+TEwqS3lMvvqTtImJmRJxNugPgO4CPkG7uVM/vgT2VLs9O7qfZoUHbT8mPD5HuvrgsIl7O22G4pPflOGvnRLAEeFLSIblcSpcyXyW+pLWAURFxB+meKhuRDs3ZasiJxJrZOaQ+gTbHA0cpXcjus6S7FCLpaElH16l/ATBA6RbJ1wBHRsSyDpZ5NfCl3PG9XaTbJEP6MoaUvF6KlfcKOR5oyR3OjwFt7fgGqR/iEUmP5ueQ7o43tq2zHThR0qOSHiYdxruJdPisbiKJdJvjI4Gr8nb4PSn5QDpJ4SZJd+Tn95AOa90dEStIt8H9bY7zOummXGfnZc8gHdKClHwn5fJZpPulrLJtgDHAj/O2fQg4LyJearBdrYn5EilmTUTpXum/i4iWvm6LWRsnEjMzq8SHtszMrBInEjMzq8SJxMzMKnEiMTOzSpxIzMysEicSMzOrxInEzMwq+R9F6kwSdyzRSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "#import ipycytoscape\n",
    "#import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "#from transformers import pipeline\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "###plotting graphs, figures, etc\n",
    "def draw_graph_pyvis(nx_graph, name, title):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        nt = Network(\"1000px\", \"1000px\", heading = title, notebook=False)\n",
    "        nt.from_nx(nx_graph)\n",
    "        #nt.show_buttons(filter_=['physics'])\n",
    "        nt.save_graph(str(name) + \"_nx.html\")\n",
    "\n",
    "\n",
    "def plots_graph_cytoscrap(G):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model = ipycytoscape.CytoscapeWidget()\n",
    "        model.graph.add_graph_from_networkx(G)\n",
    "        display(model)\n",
    "\n",
    "\n",
    "def plots_graph_hetero_edges(nodes, edge_list_A, colorA, edge_list, colorB, title, fig_name):\n",
    "        # plot graph\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(title)\n",
    "\n",
    "        G = nx.cubical_graph()\n",
    "        #pos = nx.spring_layout(G)\n",
    "        pos = nx.random_layout(G)\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodes, **options)\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=edge_list_A,\n",
    "            width=8,\n",
    "            alpha=0.5,\n",
    "            edge_color=colorA,\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=edge_list_B,\n",
    "            width=.8,\n",
    "            alpha=0.5,\n",
    "            edge_color=colorB,\n",
    "        )\n",
    "        #plt.show()\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "def plots_graph(G, title, color, fig_name):\n",
    "        # plot graph\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(title)\n",
    "\n",
    "        #pos = nx.spring_layout(G)\n",
    "        pos = nx.random_layout(G)\n",
    "\n",
    "        if color is None:\n",
    "            nx.draw_networkx(G, with_labels=True,  pos = pos, node_size = 1, alpha = 0.6, width = 0.7)\n",
    "        else:\n",
    "            nx.draw_networkx(G, with_labels=True,  pos = pos, node_size = 1, \n",
    "                             alpha = 0.6, width = 0.7, edge_color=color)\n",
    "            labels = nx.get_edge_attributes(G,'edge_weight')\n",
    "            nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)\n",
    "\n",
    "        #plt.show()\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "    \n",
    "def save_plot_dict_to_histo(data, title, labelX, labelY, labels, fig_name):\n",
    "        degreeCount = collections.Counter(data.values())\n",
    "        deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "        plt.bar(deg, cnt, width=0.80, color=\"r\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(labelX)\n",
    "        plt.ylabel(labelY)\n",
    "\n",
    "        ax.set_xticks([d for d in deg])\n",
    "        ax.set_xticklabels(deg, rotation=90, ha='center')\n",
    "        if labels is not None:\n",
    "            ax.legend(labels)\n",
    "        #plt.show(block=False)\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "\n",
    "## reading and writing data    \n",
    "def get_file_content(path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "def get_json_content(path):\n",
    "        with open(path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        data = json.loads(json_data)\n",
    "        return data\n",
    "\n",
    "def load_word2vec_model(model_path):\n",
    "        return KeyedVectors.load_word2vec_format(datapath(model_path), binary=True, unicode_errors='ignore')\n",
    "\n",
    "\n",
    "### transform data from one container to others (dict to dataframe)\n",
    "def links_to_df(links, edge_type):\n",
    "        node_list_1 = []\n",
    "        node_list_2 = []\n",
    "        edge_weight = []\n",
    "        #edge_color = []\n",
    "        #weight = []\n",
    "\n",
    "        for i in tqdm(links):\n",
    "            node_list_1.append(i.split(',')[0])\n",
    "            node_list_2.append(i.split(',')[1])\n",
    "            edge_weight.append(i.split(',')[2])\n",
    "            #edge_color.append(edge_type)\n",
    "\n",
    "        links_df = pd.DataFrame({'source': node_list_1, 'target': node_list_2, 'edge_weight': edge_weight})\n",
    "        return links_df\n",
    "\n",
    "\n",
    "##sorting algorithms\n",
    "def get_sorted_dict_by_values(data, isReverse=False):    \n",
    "        return sorted(data.items(), key=lambda x: x[1], reverse=isReverse)\n",
    "\n",
    "\n",
    "def get_common_dict_keys(dictA, dictB):    \n",
    "        \"\"\"\n",
    "            returns the keys that are common in two dictionaries\n",
    "        \"\"\"\n",
    "        common_keys = [ key for key in dictA if key in dictB]\n",
    "        return common_keys    \n",
    "\n",
    "    \n",
    "def get_combined_dict_keys(dictA, dictB):    \n",
    "        \"\"\"\n",
    "            returns the all the keys from two dictionaries\n",
    "        \"\"\"\n",
    "        keysA = list(dictA.keys())\n",
    "        keysB = list(dictB.keys())\n",
    "        combined_keys = list(set(keysA).union(keysB))\n",
    "        return combined_keys\n",
    "\n",
    "    \n",
    "def merge_two_dicts(dictA, dictB):\n",
    "        \"\"\"\n",
    "            returns the merged dictionary from the given dictionaries\n",
    "        \"\"\"\n",
    "        dictC = {}\n",
    "        for key in dictA:\n",
    "            valList = dictA.get(key)\n",
    "            if key in dictB:\n",
    "                valBList = dictB.get(key)\n",
    "                valList = valList + valBList\n",
    "            dictC[key] = valList\n",
    "        \n",
    "        for key in dictB:\n",
    "            valList = dictB.get(key)\n",
    "            dictC[key] = valList        \n",
    "        return dictC\n",
    "\n",
    "##processing parts\n",
    "def update_keyValsDict(keyValsDict, key, val):\n",
    "        \"\"\"\n",
    "            update the dictionary keyValsDict[key -> val1, val2] with the \n",
    "            given and (key, val) pair\n",
    "        \"\"\"\n",
    "        valueList = keyValsDict.get(key)\n",
    "        if valueList is None:\n",
    "            valueList = []\n",
    "        if val not in valueList:\n",
    "            valueList.append(val)\n",
    "        keyValsDict[key] = valueList\n",
    "            \n",
    "        \n",
    "def get_tweetUserID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the tweet IDs\n",
    "            {userID-> [tweetID, tweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_tweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_tweetIDs, o, t)\n",
    "        return userID_tweetIDs\n",
    "\n",
    "def get_retweetUserID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the retweet IDs\n",
    "            {userID-> [retweetID, retweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_retweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_retweetIDs, r, t)\n",
    "        return userID_retweetIDs\n",
    "    \n",
    "        \n",
    "def get_userID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the tweet/retweet IDs\n",
    "            {userID-> [TweetID, retweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_tweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_tweetIDs, r, t)\n",
    "            update_keyValsDict(userID_tweetIDs, o, t)\n",
    "        return userID_tweetIDs\n",
    "\n",
    "\n",
    "def get_userID_tweetFreq(userID_tweetID):\n",
    "        userID_tweetFreq = {}\n",
    "        max_freq = 0\n",
    "        for userID in userID_tweetID:\n",
    "            tweetIDs = userID_tweetID.get(userID)\n",
    "            freq = len(tweetIDs)\n",
    "            max_freq = max(max_freq, freq)\n",
    "            userID_tweetFreq[userID] = freq\n",
    "        return userID_tweetFreq, max_freq\n",
    "\n",
    "\n",
    "def get_message(tweetID, tweetID_msg):\n",
    "        messages = []\n",
    "        for tweetID in tweetID:\n",
    "            content = tweetID_msg[tweetID]\n",
    "            messages.append(content)\n",
    "        return ' '.join(messages)\n",
    "\n",
    "\n",
    "def messages_to_tokens(messages):\n",
    "        tokens = messages.lower().split()\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def lexicon_based_features(lexicons, messages):\n",
    "        features = [0] * len(lexicons)\n",
    "        tokens = messages_to_tokens(messages)\n",
    "        tokens_freq = collections.Counter(tokens)\n",
    "\n",
    "        for ldx in range(0, len(lexicons)):\n",
    "            lexicon = lexicons[ldx]\n",
    "            if lexicon in tokens:\n",
    "                freq = tokens_freq.get(lexicon)\n",
    "                features[ldx] = freq\n",
    "        return features\n",
    "\n",
    "\n",
    "def get_userID_lexicon_representation(userID_tweetIDs, tweetID_msg, lexicons):    \n",
    "        representations = []\n",
    "        lexicons_lower = [lexicon.lower() for lexicon in lexicons]\n",
    "        user_IDs = []\n",
    "        for userID in userID_tweetIDs:\n",
    "            tweetID = userID_tweetIDs.get(userID)\n",
    "            messages = get_message(tweetID, tweetID_msg)\n",
    "            features = lexicon_based_features(lexicons_lower, messages)    \n",
    "            representations.append(features)\n",
    "            user_IDs.append(userID)\n",
    "        return user_IDs, representations\n",
    "    \n",
    "def get_userID_combined_lexicon_word2vec_representation(userIDs, msg_lexicon_repr, lexicons, word2vec_model):\n",
    "        msg_lexicon_word2vec_repr = []\n",
    "        \n",
    "        for udx in range(0, len(userIDs)):\n",
    "            userID = userIDs[udx]\n",
    "            lexicon_repr = msg_lexicon_repr[udx]\n",
    "\n",
    "            sum_freq = sum(lexicon_repr)\n",
    "            if sum_freq <= 0:\n",
    "                msg_lexicon_word2vec_repr.append(np.zeros_like(word2vec_model.wv[\"the\"]))\n",
    "                continue\n",
    "            \n",
    "            lexicons_vector = [] \n",
    "            for ldx in range(0, len(lexicons)):\n",
    "                lexicon = lexicons[ldx]\n",
    "                freq = lexicon_repr[ldx]\n",
    "                \n",
    "                if freq > 0:\n",
    "                    if lexicon in word2vec_model:\n",
    "                        lexicon_vector = word2vec_model[lexicon]\n",
    "                        lexicons_vector.append(freq * lexicon_vector)\n",
    "                    else:\n",
    "                        lexicon_vector = np.zeros_like(word2vec_model[\"the\"])\n",
    "                        lexicons_vector.append(freq * lexicon_vector)\n",
    "            msg_lexicon_word2vec_repr.append(np.sum(lexicons_vector, axis=0)/sum_freq)\n",
    "        return msg_lexicon_word2vec_repr\n",
    "    \n",
    "\n",
    "def get_normalized_representation(lex_repr):\n",
    "        \"\"\"\n",
    "            returns the normalized lexicon representaiton\n",
    "        \"\"\"\n",
    "        normalized_representations = normalize(lex_repr, axis=0, norm='max')        \n",
    "        #print (\"After normalization: {}\".format(len(normalized_representations)))\n",
    "        #print (\"After normalization: {}\".format(len(normalized_representations[0])))\n",
    "        return normalized_representations\n",
    "    \n",
    "    \n",
    "def get_userID_lex_score(userID_lexicon_representation):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        userIDs = userID_lexicon_representation.keys()\n",
    "        userID_lexicon_score = {}\n",
    "        \n",
    "        max_score = 0\n",
    "        for userID in userIDs:\n",
    "            representation = userID_lexicon_representation[userID]\n",
    "            score = np.sum(np.array(representation)>0)\n",
    "            max_score = max(max_score, score)\n",
    "            userID_lexicon_score[userID] = score    \n",
    "        return userID_lexicon_score, max_score\n",
    "\n",
    "\n",
    "def get_candidate_userID(userID_lex_score_sorted, threshold):\n",
    "        \"\"\"\n",
    "            returns the candidate userID based on the threshold\n",
    "        \"\"\"\n",
    "        candidate_userID = [item[0] for item in userID_lex_score_sorted if item[1]>=threshold]\n",
    "        return candidate_userID\n",
    "\n",
    "    \n",
    "def get_candidate_userID_lex_repr(candidate_userID, userID_lex_repr):\n",
    "        \"\"\"\n",
    "            returns the cadidate userID with its representation\n",
    "        \"\"\"\n",
    "        candidate_userID_lex_repr = {}\n",
    "        for userID in candidate_userID:\n",
    "            candidate_userID_lex_repr[userID] = userID_lex_repr[userID]\n",
    "        return candidate_userID_lex_repr\n",
    "    \n",
    "def dot_length_similarity(data_matrix, lexicon):\n",
    "        \"\"\"\n",
    "            returns the normalized data using the function as \n",
    "            N = dot(A, B)/len(A)\n",
    "        \"\"\"    \n",
    "        length = len(lexicon)\n",
    "        print (length)\n",
    "        D = np.array(data_matrix)\n",
    "        Dt = D.T\n",
    "        S = np.dot(D, Dt)\n",
    "        NS = S/length\n",
    "        return NS\n",
    "    \n",
    "\n",
    "def get_min_max_sim(users_sim):\n",
    "        \"\"\"\n",
    "            return the minimum and maximum similarity score\n",
    "        \"\"\"\n",
    "        min_score = 1\n",
    "        max_score = 0\n",
    "        for item in tqdm(range(0, len(users_sim))):\n",
    "            sim_vec = users_sim[item]\n",
    "            ms = np.min(sim_vec)\n",
    "            MS = np.max(sim_vec)\n",
    "            min_score = min(min_score, ms)\n",
    "            max_score = max(max_score, MS)\n",
    "        return min_score, max_score\n",
    "    \n",
    "    \n",
    "def get_implicit_links(candidate_userID, user_to_user_sim, link_threshold):\n",
    "        im_links = []\n",
    "        max_weight = 0\n",
    "        for udx in tqdm(range(0, len(candidate_userID))):\n",
    "            node1 = candidate_userID[udx]\n",
    "            neighbors = user_to_user_sim[udx]\n",
    "\n",
    "            #devide by the lexicon size\n",
    "\n",
    "            neighbors_decision = np.array(neighbors[(udx+1):])>=link_threshold\n",
    "            neighbors_candidates = np.array(candidate_userID[(udx+1):])[neighbors_decision]\n",
    "            neighbors_weight = np.array(neighbors[(udx+1):])[neighbors_decision]\n",
    "\n",
    "            for ndx in range(0, len(neighbors_candidates)):\n",
    "                neighbor = neighbors_candidates[ndx]\n",
    "                weight = round(neighbors_weight[ndx], 3)\n",
    "                max_weight = max(max_weight, weight)\n",
    "                im_links.append(str(node1)+','+str(neighbor)+','+str(weight))           \n",
    "            #vdx = udx + 1\n",
    "            #while vdx<len(candidate_userID):\n",
    "            #    node2 = candidate_userID[vdx]\n",
    "            #    weight = neighbors[vdx]\n",
    "            #    max_weight = max(max_weight, weight)\n",
    "            #    if weight>=link_threshold:\n",
    "            #         im_links.append(str(node1)+','+str(node2)+','+str(weight))\n",
    "            #    vdx = vdx + 1\n",
    "        return im_links, max_weight\n",
    "\n",
    "\n",
    "def get_explicit_links(retweetUID_tweetUID_tweetID):\n",
    "        ex_links = []\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            node1 = parts[0]\n",
    "            node2 = parts[1]\n",
    "            t = parts[2]   \n",
    "            weight = 1\n",
    "            ex_links.append(str(node1)+','+str(node2)+','+str(weight))\n",
    "        return ex_links\n",
    "\n",
    "\n",
    "def get_node_degree(G):\n",
    "        \"\"\"\n",
    "            returns the nodes with its degree from the graph\n",
    "        \"\"\"\n",
    "        node_sequence = [n for n, d in G.degree()]\n",
    "        degree_sequence = [d for n, d in G.degree()]\n",
    "        max_degree = 0\n",
    "        if len(degree_sequence)>0:\n",
    "            max_degree = max(degree_sequence)\n",
    "        node_degree = dict(zip(node_sequence, degree_sequence))\n",
    "        return node_degree, max_degree\n",
    "\n",
    "def get_nodes_with_degree(G, degree):\n",
    "        return [node for node,degree in dict(G.degree()).items() if degree < 2]\n",
    "\n",
    "\n",
    "print (\"setting up the data path ...\")\n",
    "data_dir = \"/projets/sig/mullah/nlp/fgpi/\"\n",
    "figures_path ='../figures/'\n",
    "graph_dir = \"/projets/sig/mullah/nlp/fgpi/graph\"\n",
    "lexicon_name='Radical'\n",
    "w2v_model = \"w2v_google\"\n",
    "#w2v_model = \"w2v_twitter\"\n",
    "\n",
    "print (\"Approach ..\")\n",
    "approach = \"Lexicon_word2vec\"\n",
    "\n",
    "#newretweetuserid_neworiginaluserid_path = os.path.join(data_dir, 'data/processed/20000_UserRetweetID_UserOriginalID_NewID.txt')\n",
    "tweetID_msg_path = os.path.join(data_dir, 'data/processed/20000_id_OriginalTweet.json')\n",
    "#retweetUserID_tweetUserID_tweetID_path = os.path.join(data_dir, 'data/processed/20000_UserRetweet_UserOriginal_idOriginalTweet.txt')\n",
    "retweetUserID_tweetUserID_tweetID_path = os.path.join(data_dir, 'data/processed/20000_RetweetUserNewID_TweetUserNewID_TweetID.txt')\n",
    "\n",
    "lexicon_path = os.path.join(data_dir, 'lexicons/'+lexicon_name)\n",
    "if w2v_model == \"w2v_twitter\":\n",
    "    word2vec_model_path = '/projets/sig/mullah/ir/data/word_embedding/pretrain/twitter/word2vec_twitter_tokens.bin'\n",
    "else:\n",
    "    word2vec_model_path = '/projets/sig/mullah/ir/data/word_embedding/pretrain/word2vec/googlenews/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "    \n",
    "print (\"Loading word2vec models ...\")\n",
    "#word2vec_model = load_word2vec_model(word2vec_google_model_path)\n",
    "word2vec_model = load_word2vec_model(word2vec_model_path)\n",
    "print (\"Done.\")\n",
    "\n",
    "print (\"Loading the tweetID and messages ...\")\n",
    "tweetID_msg = get_json_content(tweetID_msg_path)\n",
    "print (\"Total tweets: {}\".format(len(tweetID_msg)))\n",
    "\n",
    "print (\"Loading the retweerUser, tweetUser, and tweetId ...\")\n",
    "retweetUserID_tweetUserID_tweetID = get_file_content(retweetUserID_tweetUserID_tweetID_path)\n",
    "print (\"Total retweets: {}\".format(len(retweetUserID_tweetUserID_tweetID)))\n",
    "\n",
    "print (\"Loading the lexicon file ...\")\n",
    "lexicon = get_file_content(lexicon_path)\n",
    "print (\"Total jargons in lexicon: {}\".format(len(lexicon)))\n",
    "\n",
    "print (\"Extracting tweetUser and tweetId ...\")\n",
    "tweetUserID_tweetId = get_tweetUserID_tweetIDs(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total users who tweeted: {}\".format(len(tweetUserID_tweetId)))\n",
    "\n",
    "print (\"Extracting retweetUser and tweetId ...\")\n",
    "retweetUserID_tweetId = get_retweetUserID_tweetIDs(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total users who retweeted: {}\".format(len(retweetUserID_tweetId)))\n",
    "\n",
    "print (\"Getting both tweet and retweet users ...\")\n",
    "bothTweetRetweetUserID = get_common_dict_keys(tweetUserID_tweetId, retweetUserID_tweetId)\n",
    "print (\"Total users who both tweeted and retweeted: {}\".format(len(bothTweetRetweetUserID)))\n",
    "\n",
    "print (\"Getting all users either tweeter or retweeter ...\")\n",
    "userID_tweetIDs = merge_two_dicts(tweetUserID_tweetId, retweetUserID_tweetId)\n",
    "print (\"Total user who tweeted or retweeted: {}\".format(len(userID_tweetIDs)))\n",
    "\n",
    "print (\"Ploting the data ...\")\n",
    "userID_freq, max_freq = get_userID_tweetFreq(userID_tweetIDs)\n",
    "#save_plot_dict_to_histo(userID_freq, \"Distribution of tweet/retweet poster by users\", \"No. of tweets/retweets\", \"No. of users\", \n",
    "#                        ['Max no. of tweets/retweets: '+str(max_freq)], figures_path + \"Users_Tweets_Distribution.png\")\n",
    "save_plot_dict_to_histo(userID_freq, \"Distribution of tweet/retweet posted by users\", \"No. of tweets/retweets\", \"No. of users\", \n",
    "                        None, figures_path + approach + '_' +\"Users_Tweets_Distribution.png\")\n",
    "print (\"done.\")\n",
    "\n",
    "print (\"Estimating users lexicon representation ...\")\n",
    "userIDs, msg_lexicon_repr = get_userID_lexicon_representation(userID_tweetIDs, tweetID_msg, lexicon)\n",
    "print (\"done.\")\n",
    "\n",
    "print (\"Combining users lexicon and word2vec representation ...\")\n",
    "msg_lexicon_word2vec_repr = get_userID_combined_lexicon_word2vec_representation(userIDs, msg_lexicon_repr, lexicon, word2vec_model)\n",
    "print (\"done.\")\n",
    "\n",
    "#print (\"Normalizing user's lexicon based represention ...\")\n",
    "#lexicon_repr_normalized = get_normalized_representation(lexicon_repr)\n",
    "#print (\"done.\")\n",
    "\n",
    "#print (\"Users used jargon from the lexicon...\")\n",
    "#userID_lex_score, max_jargon_used = get_userID_lex_score(userID_lex_repr_normalized)\n",
    "#save_plot_dict_to_histo(userID_lex_score, \"Distribution of jargon in tweets/retweets of users\", \"Numbers of jargon used\", \n",
    "#                        \"No. of users\", ['Max jargon used: '+str(max_jargon_used)], figures_path + \"Users_Radical_Scores_Distribution.png\")\n",
    "\n",
    "#print (\"Maximum jargon used: {}\".format(max_jargon_used))\n",
    "\n",
    "#print (\"Setting up the parameters ...\")\n",
    "#link_weight_threshold = 1\n",
    "\n",
    "print ('Loading explicit links ...')\n",
    "ex_links = get_explicit_links(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total explicit links: {}\".format(len(ex_links)))\n",
    "\n",
    "edge_color = 'b'\n",
    "ex_links_df = links_to_df(ex_links, edge_color)\n",
    "G_explicit_link = nx.from_pandas_edgelist(ex_links_df, \"source\", \"target\", create_using=nx.Graph())\n",
    "#print ('Drawing the graph of explicit links only ...')\n",
    "\n",
    "degree = 1\n",
    "nodes_degree = get_nodes_with_degree(G_explicit_link, degree)\n",
    "print (\"Total nodes with degree at most {} is {}\".format(degree, len(nodes_degree)))\n",
    "\n",
    "print (\"Removing nodes ....\")\n",
    "G_explicit_link.remove_nodes_from(nodes_degree)\n",
    "print (\"done.\")\n",
    "\n",
    "#edge_color = 'b'\n",
    "#plots_graph(G_explicit_link, \"Graph of users with explicit links\", edge_color,\n",
    "#            figures_path + \"Graph_of_users_with_explicit_links.png\")\n",
    "#plots_graph(G_explicit_link, \"Graph of users with explicit links\", edge_color,\n",
    "#            figures_path + \"Graph_of_users_with_explicit_links.png\")\n",
    "draw_graph_pyvis(G_explicit_link, figures_path + \"Graph_of_users_with_explicit_links\", \"Graph_of_users_with_explicit_links\")\n",
    "nx.write_gpickle(G_explicit_link, graph_dir + \"/Graph_of_users_with_explicit_links\")\n",
    "\n",
    "#explit_edges = nx.to_edgelist(G_explicit_link)\n",
    "#nodes = nx.nodes\n",
    "\n",
    "print (\"Transforming the candidate users and its representation as data frame\")\n",
    "msg_lexicon_word2vec_repr_DF = pd.DataFrame(msg_lexicon_word2vec_repr)\n",
    "print(msg_lexicon_word2vec_repr_DF.info())\n",
    "    \n",
    "print (\"Computing dot product divide by the length...\")\n",
    "users_Sim = dot_length_similarity(msg_lexicon_word2vec_repr_DF, lexicon)\n",
    "print (\"Users x Users similarity dim: {},{}\".format(len(users_Sim),len(users_Sim[0])))\n",
    "min_sim, max_sim = get_min_max_sim(users_Sim)\n",
    "print (\"Min_sim:{}, Max_sim:{}\".format(min_sim, max_sim))\n",
    "print (\"User:{}, Similarities: {}\".format(userIDs[0], users_Sim[0]))\n",
    "\n",
    "#print (\"Computing user similarity ...\")\n",
    "#user_to_user_sim = squareform(pdist(candidate_userID_representation_DF.T, metric='cosine'))\n",
    "#users_Sim = cosine_similarity(lexicon_repr_normalized_DF)\n",
    "#print (\"{},{}\".format(len(users_Sim),len(users_Sim[0])))\n",
    "#print (users_Sim)\n",
    "\n",
    "#for num_jargon in range(1, max_jargon_used):\n",
    "min_sim = max(min_sim, 0)\n",
    "dif = max_sim - min_sim\n",
    "inc = dif/10\n",
    "\n",
    "#for step in range(1, 11):\n",
    "for step in range(4, 5):\n",
    "    #edge_weight = step/10\n",
    "    edge_weight = min_sim + step * inc\n",
    "    print (\"Assessing the candidate users with edge weight {} for step {}\".format(edge_weight, step))\n",
    "\n",
    "    #userID_lex_score_sorted = get_sorted_dict_by_values(userID_lex_score, True)\n",
    "    #candidate_userID = get_candidate_userID(userID_lex_score_sorted, edge_weight)\n",
    "    #print (\"Total candidate users: {}\".format(len(candidate_userID)))\n",
    "\n",
    "    #print (\"Representaiton of candidate users ....\")\n",
    "    #candidate_userID_lex_repr = get_candidate_userID_lex_repr(candidate_userID, userID_lex_repr_normalized)\n",
    "       \n",
    "    print (\"Generating implicit links ...\")\n",
    "    im_links, max_weight = get_implicit_links(userIDs, users_Sim, edge_weight) \n",
    "    print (\"Total implicit links: {} and maximum weight: {}\".format(len(im_links), max_weight))\n",
    "\n",
    "    print (\"Implicit links to data frame ...\")\n",
    "    edge_color = \"r\"\n",
    "    im_links_df = links_to_df(im_links, edge_color)\n",
    "    \n",
    "    print ('writing to file ...')\n",
    "    im_graph_path = graph_dir + \"/\" + approach + '_' + 'implicit_' + lexicon_name + '_' + str(step)\n",
    "    im_links_df.to_csv(im_graph_path)\n",
    "    print ('done')\n",
    "\n",
    "    print (\"Creating graph ...\")\n",
    "    G_implicit_link = nx.from_pandas_edgelist(im_links_df, edge_attr=\"edge_weight\", \n",
    "                                              create_using=nx.Graph())\n",
    "    node_degree, max_degree = get_node_degree(G_implicit_link)\n",
    "    if len(node_degree) < 2:\n",
    "        continue\n",
    "    #plots_graph_cytoscrap(G_implicit_link)\n",
    "    print (\"Drawing the implicit graph using pyvis ...\")\n",
    "    draw_graph_pyvis(G_implicit_link, figures_path + approach + '_' + \"Graph_of_users_with_implicit_links_\" + str(step), \"Graph_of_users_with_implicit_links_\" + str(step))\n",
    "    \n",
    "    \"\"\"\n",
    "    print ('Degree distribution of users in the graph of implicit links ...')\n",
    "    save_plot_dict_to_histo(node_degree, \"Degree distribution of users with implicit links\", \"Degree\", \n",
    "                            \"No. of users\", ['Max degree: '+str(max_degree)], figures_path + 'Graph_implicit_degree_'+str(edge_weight)+'.png')\n",
    "\n",
    "    print ('Drawing the graph of implicit links only ...')\n",
    "    plots_graph(G_implicit_link, \"Graph of users with implicit links for edge weight \"+str(edge_weight), \n",
    "                edge_color, figures_path + \"Graph_of_users_with_implicit_links_for_\"+str(edge_weight)+\".png\")\n",
    "\n",
    "    \n",
    "    implicit_edges = nx.to_edgelist(G_implicit_link)\n",
    "    #Gexp_imp = F = nx.compose(G_explicit_link, G_implicit_link)\n",
    "    #print ('Drawing the graph of implicit links only ...')\n",
    "    #plots_graph(G_implicit_link, \"Graph of users with implicit links for \"+str(num_jargon)+\"\", \n",
    "    #            edge_color, figures_path + \"Graph_of_users_with_implicit_links_for_\"+str(num_jargon)+\".png\")\n",
    "    \n",
    "    #print (\"Drawing the superimposed explicit and implicit graphs ... \")\n",
    "    #plots_graph_hetero_edges(nodes, explit_edges, 'b',  implicit_edges, 'r', \n",
    "    #                         \"Graph of users with explicit and implicit links for \"+str(num_jargon)+\"jargon\", \n",
    "    #                        figures_path + \"Graph_of_users_with_explicit_and_implicit_links_for_\"+str(num_jargon)+\"_lexicons.png\")\n",
    "    \"\"\"\n",
    "    \n",
    "    #integrating two graphs\n",
    "    extended_graph = nx.Graph()\n",
    "    extended_graph.add_edges_from(G_explicit_link.edges, color=\"blue\")\n",
    "    extended_graph.add_edges_from(G_implicit_link.edges, color=\"red\")\n",
    "    print (\"Drawing the combined graph (explicit and implict) using pyvis ...\")\n",
    "    draw_graph_pyvis(extended_graph, figures_path + approach + '_' + \"Graph_of_users_with_explicit_and_implicit_links_\" + str(step), \"Graph_of_users_with_explicit_and_implicit_links_\" + str(step))\n",
    "    nx.write_gpickle(extended_graph, graph_dir + \"/\" + approach + '_' + \"Graph_of_users_with_explicit_and_implicit_links_\" + str(step))\n",
    "    \n",
    "print ('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(TC11N)",
   "language": "python",
   "name": "e36t11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
