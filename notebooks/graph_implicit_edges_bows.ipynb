{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the data path ...\n",
      "Loading the tweetID and messages ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 926/926 [00:00<00:00, 916452.45it/s]\n",
      "100%|██████████| 926/926 [00:00<00:00, 948919.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 423\n",
      "Loading the retweerUser, tweetUser, and tweetId ...\n",
      "Total retweets: 926\n",
      "Loading the lexicon file ...\n",
      "Total jargons in lexicon: 189\n",
      "Extracting tweetUser and tweetId ...\n",
      "Total users who tweeted: 361\n",
      "Extracting retweetUser and tweetId ...\n",
      "Total users who retweeted: 329\n",
      "Getting both tweet and retweet users ...\n",
      "Total users who both tweeted and retweeted: 0\n",
      "Getting all users either tweeter or retweeter ...\n",
      "Total user who tweeted or retweeted: 690\n",
      "Ploting the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 926/926 [00:00<00:00, 818185.28it/s]\n",
      "100%|██████████| 926/926 [00:00<00:00, 596425.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Estimating users lexicon representation ...\n",
      "done.\n",
      "Normalizing user's lexicon based represention ...\n",
      "done.\n",
      "Loading explicit links ...\n",
      "Total explicit links: 926\n",
      "Total nodes with degree at most 1 is 403\n",
      "Removing nodes ....\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 98521.52it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 27144.29it/s]\n",
      "100%|██████████| 219/219 [00:00<00:00, 729589.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the candidate users and its representation as data frame\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Columns: 189 entries, 0 to 188\n",
      "dtypes: float64(189)\n",
      "memory usage: 1019.0 KB\n",
      "None\n",
      "Computing dot product divide by the length...\n",
      "189\n",
      "690,690\n",
      "0.0,0.047619047619047616\n",
      "Assessing the candidate users with edge weight 0.0047619047619047615 for step 1\n",
      "Generating implicit links ...\n",
      "Total implicit links: 219 and maximum weight: 0.027\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 28929.70it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 31570.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the candidate users with edge weight 0.009523809523809523 for step 2\n",
      "Generating implicit links ...\n",
      "Total implicit links: 14 and maximum weight: 0.027\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 29481.99it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 154866.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the candidate users with edge weight 0.014285714285714284 for step 3\n",
      "Generating implicit links ...\n",
      "Total implicit links: 12 and maximum weight: 0.027\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 29298.13it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 214592.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the candidate users with edge weight 0.019047619047619046 for step 4\n",
      "Generating implicit links ...\n",
      "Total implicit links: 11 and maximum weight: 0.027\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 30009.33it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 182361.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the candidate users with edge weight 0.023809523809523808 for step 5\n",
      "Generating implicit links ...\n",
      "Total implicit links: 10 and maximum weight: 0.027\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 31290.96it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 30204.77it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 30115.19it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessing the candidate users with edge weight 0.028571428571428567 for step 6\n",
      "Generating implicit links ...\n",
      "Total implicit links: 0 and maximum weight: 0\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n",
      "Assessing the candidate users with edge weight 0.03333333333333333 for step 7\n",
      "Generating implicit links ...\n",
      "Total implicit links: 0 and maximum weight: 0\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n",
      "Assessing the candidate users with edge weight 0.03809523809523809 for step 8\n",
      "Generating implicit links ...\n",
      "Total implicit links: 0 and maximum weight: 0\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 690/690 [00:00<00:00, 29280.35it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 690/690 [00:00<00:00, 30705.66it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Creating graph ...\n",
      "Assessing the candidate users with edge weight 0.04285714285714285 for step 9\n",
      "Generating implicit links ...\n",
      "Total implicit links: 0 and maximum weight: 0\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n",
      "done\n",
      "Creating graph ...\n",
      "Assessing the candidate users with edge weight 0.047619047619047616 for step 10\n",
      "Generating implicit links ...\n",
      "Total implicit links: 0 and maximum weight: 0\n",
      "Implicit links to data frame ...\n",
      "writing to file ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Creating graph ...\n",
      "done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "#import ipycytoscape\n",
    "#import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "#from transformers import pipeline\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "###plotting graphs, figures, etc\n",
    "def draw_graph_pyvis(nx_graph, name, title):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        nt = Network(\"1000px\", \"1000px\", heading = title, notebook=False)\n",
    "        nt.from_nx(nx_graph)\n",
    "        #nt.show_buttons(filter_=['physics'])\n",
    "        nt.save_graph(str(name) + \"_nx.html\")\n",
    "\n",
    "\n",
    "def plots_graph_cytoscrap(G):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        model = ipycytoscape.CytoscapeWidget()\n",
    "        model.graph.add_graph_from_networkx(G)\n",
    "        display(model)\n",
    "\n",
    "\n",
    "def plots_graph_hetero_edges(nodes, edge_list_A, colorA, edge_list, colorB, title, fig_name):\n",
    "        # plot graph\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(title)\n",
    "\n",
    "        G = nx.cubical_graph()\n",
    "        #pos = nx.spring_layout(G)\n",
    "        pos = nx.random_layout(G)\n",
    "\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=nodes, **options)\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=edge_list_A,\n",
    "            width=8,\n",
    "            alpha=0.5,\n",
    "            edge_color=colorA,\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G,\n",
    "            pos,\n",
    "            edgelist=edge_list_B,\n",
    "            width=.8,\n",
    "            alpha=0.5,\n",
    "            edge_color=colorB,\n",
    "        )\n",
    "        #plt.show()\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "def plots_graph(G, title, color, fig_name):\n",
    "        # plot graph\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(title)\n",
    "\n",
    "        #pos = nx.spring_layout(G)\n",
    "        pos = nx.random_layout(G)\n",
    "\n",
    "        if color is None:\n",
    "            nx.draw_networkx(G, with_labels=True,  pos = pos, node_size = 1, alpha = 0.6, width = 0.7)\n",
    "        else:\n",
    "            nx.draw_networkx(G, with_labels=True,  pos = pos, node_size = 1, \n",
    "                             alpha = 0.6, width = 0.7, edge_color=color)\n",
    "            labels = nx.get_edge_attributes(G,'edge_weight')\n",
    "            nx.draw_networkx_edge_labels(G,pos,edge_labels=labels)\n",
    "\n",
    "        #plt.show()\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "    \n",
    "def save_plot_dict_to_histo(data, title, labelX, labelY, labels, fig_name):\n",
    "        degreeCount = collections.Counter(data.values())\n",
    "        deg, cnt = zip(*degreeCount.items())\n",
    "\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "        plt.bar(deg, cnt, width=0.80, color=\"r\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(labelX)\n",
    "        plt.ylabel(labelY)\n",
    "\n",
    "        ax.set_xticks([d for d in deg])\n",
    "        ax.set_xticklabels(deg, rotation=90, ha='center')\n",
    "        if labels is not None:\n",
    "            ax.legend(labels)\n",
    "        #plt.show(block=False)\n",
    "        plt.savefig(fig_name, format=\"PNG\")\n",
    "        #plt.close()\n",
    "\n",
    "\n",
    "## reading and writing data    \n",
    "def get_file_content(path):\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return lines\n",
    "\n",
    "def get_json_content(path):\n",
    "        with open(path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        data = json.loads(json_data)\n",
    "        return data\n",
    "\n",
    "\n",
    "### transform data from one container to others (dict to dataframe)\n",
    "def links_to_df(links, edge_type):\n",
    "        node_list_1 = []\n",
    "        node_list_2 = []\n",
    "        edge_weight = []\n",
    "        #edge_color = []\n",
    "        #weight = []\n",
    "\n",
    "        for i in tqdm(links):\n",
    "            node_list_1.append(i.split(',')[0])\n",
    "            node_list_2.append(i.split(',')[1])\n",
    "            edge_weight.append(i.split(',')[2])\n",
    "            #edge_color.append(edge_type)\n",
    "\n",
    "        links_df = pd.DataFrame({'source': node_list_1, 'target': node_list_2, 'edge_weight': edge_weight})\n",
    "        return links_df\n",
    "\n",
    "\n",
    "##sorting algorithms\n",
    "def get_sorted_dict_by_values(data, isReverse=False):    \n",
    "        return sorted(data.items(), key=lambda x: x[1], reverse=isReverse)\n",
    "\n",
    "\n",
    "def get_common_dict_keys(dictA, dictB):    \n",
    "        \"\"\"\n",
    "            returns the keys that are common in two dictionaries\n",
    "        \"\"\"\n",
    "        common_keys = [ key for key in dictA if key in dictB]\n",
    "        return common_keys    \n",
    "\n",
    "    \n",
    "def get_combined_dict_keys(dictA, dictB):    \n",
    "        \"\"\"\n",
    "            returns the all the keys from two dictionaries\n",
    "        \"\"\"\n",
    "        keysA = list(dictA.keys())\n",
    "        keysB = list(dictB.keys())\n",
    "        combined_keys = list(set(keysA).union(keysB))\n",
    "        return combined_keys\n",
    "\n",
    "    \n",
    "def merge_two_dicts(dictA, dictB):\n",
    "        \"\"\"\n",
    "            returns the merged dictionary from the given dictionaries\n",
    "        \"\"\"\n",
    "        dictC = {}\n",
    "        for key in dictA:\n",
    "            valList = dictA.get(key)\n",
    "            if key in dictB:\n",
    "                valBList = dictB.get(key)\n",
    "                valList = valList + valBList\n",
    "            dictC[key] = valList\n",
    "        \n",
    "        for key in dictB:\n",
    "            valList = dictB.get(key)\n",
    "            dictC[key] = valList        \n",
    "        return dictC\n",
    "\n",
    "##processing parts\n",
    "def update_keyValsDict(keyValsDict, key, val):\n",
    "        \"\"\"\n",
    "            update the dictionary keyValsDict[key -> val1, val2] with the \n",
    "            given and (key, val) pair\n",
    "        \"\"\"\n",
    "        valueList = keyValsDict.get(key)\n",
    "        if valueList is None:\n",
    "            valueList = []\n",
    "        if val not in valueList:\n",
    "            valueList.append(val)\n",
    "        keyValsDict[key] = valueList\n",
    "            \n",
    "        \n",
    "def get_tweetUserID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the tweet IDs\n",
    "            {userID-> [tweetID, tweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_tweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_tweetIDs, o, t)\n",
    "        return userID_tweetIDs\n",
    "\n",
    "def get_retweetUserID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the retweet IDs\n",
    "            {userID-> [retweetID, retweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_retweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_retweetIDs, r, t)\n",
    "        return userID_retweetIDs\n",
    "    \n",
    "        \n",
    "def get_userID_tweetIDs(retweetUID_tweetUID_tweetID):\n",
    "        \"\"\"\n",
    "            return the users with the tweet/retweet IDs\n",
    "            {userID-> [TweetID, retweetID, ...]}\n",
    "        \"\"\"\n",
    "        userID_tweetIDs = {}\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            r, o, t = parts[0], parts[1], parts[2]   \n",
    "        \n",
    "            update_keyValsDict(userID_tweetIDs, r, t)\n",
    "            update_keyValsDict(userID_tweetIDs, o, t)\n",
    "        return userID_tweetIDs\n",
    "\n",
    "\n",
    "def get_userID_tweetFreq(userID_tweetID):\n",
    "        userID_tweetFreq = {}\n",
    "        max_freq = 0\n",
    "        for userID in userID_tweetID:\n",
    "            tweetIDs = userID_tweetID.get(userID)\n",
    "            freq = len(tweetIDs)\n",
    "            max_freq = max(max_freq, freq)\n",
    "            userID_tweetFreq[userID] = freq\n",
    "        return userID_tweetFreq, max_freq\n",
    "\n",
    "\n",
    "def get_message(tweetID, tweetID_msg):\n",
    "        messages = []\n",
    "        for tweetID in tweetID:\n",
    "            content = tweetID_msg[tweetID]\n",
    "            messages.append(content)\n",
    "        return ' '.join(messages)\n",
    "\n",
    "\n",
    "def messages_to_tokens(messages):\n",
    "        tokens = messages.lower().split()\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def lexicon_based_features(lexicons, messages):\n",
    "        features = [0] * len(lexicons)\n",
    "        tokens = messages_to_tokens(messages)\n",
    "        tokens_freq = collections.Counter(tokens)\n",
    "\n",
    "        for ldx in range(0, len(lexicons)):\n",
    "            lexicon = lexicons[ldx]\n",
    "            if lexicon in tokens:\n",
    "                freq = tokens_freq.get(lexicon)\n",
    "                features[ldx] = freq\n",
    "        return features\n",
    "\n",
    "\n",
    "def get_userID_lex_repr(userID_tweetIDs, tweetID_msg, lexicons):    \n",
    "        representations = []\n",
    "        lexicons_lower = [lexicon.lower() for lexicon in lexicons]\n",
    "        user_IDs = []\n",
    "        for userID in userID_tweetIDs:\n",
    "            tweetID = userID_tweetIDs.get(userID)\n",
    "            messages = get_message(tweetID, tweetID_msg)\n",
    "            features = lexicon_based_features(lexicons_lower, messages)    \n",
    "            representations.append(features)\n",
    "            user_IDs.append(userID)\n",
    "        return user_IDs, representations\n",
    "\n",
    "def get_normalized_representation(lex_repr):\n",
    "        \"\"\"\n",
    "            returns the normalized lexicon representaiton\n",
    "        \"\"\"\n",
    "        normalized_representations = normalize(lex_repr, axis=0, norm='max')        \n",
    "        #print (\"After normalization: {}\".format(len(normalized_representations)))\n",
    "        #print (\"After normalization: {}\".format(len(normalized_representations[0])))\n",
    "        return normalized_representations\n",
    "    \n",
    "    \n",
    "def get_userID_lex_score(userID_lexicon_representation):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        userIDs = userID_lexicon_representation.keys()\n",
    "        userID_lexicon_score = {}\n",
    "        \n",
    "        max_score = 0\n",
    "        for userID in userIDs:\n",
    "            representation = userID_lexicon_representation[userID]\n",
    "            score = np.sum(np.array(representation)>0)\n",
    "            max_score = max(max_score, score)\n",
    "            userID_lexicon_score[userID] = score    \n",
    "        return userID_lexicon_score, max_score\n",
    "\n",
    "\n",
    "def get_candidate_userID(userID_lex_score_sorted, threshold):\n",
    "        \"\"\"\n",
    "            returns the candidate userID based on the threshold\n",
    "        \"\"\"\n",
    "        candidate_userID = [item[0] for item in userID_lex_score_sorted if item[1]>=threshold]\n",
    "        return candidate_userID\n",
    "\n",
    "    \n",
    "def get_candidate_userID_lex_repr(candidate_userID, userID_lex_repr):\n",
    "        \"\"\"\n",
    "            returns the cadidate userID with its representation\n",
    "        \"\"\"\n",
    "        candidate_userID_lex_repr = {}\n",
    "        for userID in candidate_userID:\n",
    "            candidate_userID_lex_repr[userID] = userID_lex_repr[userID]\n",
    "        return candidate_userID_lex_repr\n",
    "    \n",
    "def dot_length_similarity(data_matrix, lexicon):\n",
    "        \"\"\"\n",
    "            returns the normalized data using the function as \n",
    "            N = dot(A, B)/len(A)\n",
    "        \"\"\"    \n",
    "        length = len(lexicon)\n",
    "        print (length)\n",
    "        D = np.array(data_matrix)\n",
    "        Dt = D.T\n",
    "        S = np.dot(D, Dt)\n",
    "        NS = S/length\n",
    "        return NS\n",
    "    \n",
    "\n",
    "def get_min_max_sim(users_sim):\n",
    "        \"\"\"\n",
    "            return the minimum and maximum similarity score\n",
    "        \"\"\"\n",
    "        min_score = 1\n",
    "        max_score = 0\n",
    "        for item in tqdm(range(0, len(users_sim))):\n",
    "            sim_vec = users_sim[item]\n",
    "            ms = np.min(sim_vec)\n",
    "            MS = np.max(sim_vec)\n",
    "            min_score = min(min_score, ms)\n",
    "            max_score = max(max_score, MS)\n",
    "        return min_score, max_score\n",
    "    \n",
    "    \n",
    "def get_implicit_links(candidate_userID, user_to_user_sim, link_threshold):\n",
    "        im_links = []\n",
    "        max_weight = 0\n",
    "        for udx in tqdm(range(0, len(candidate_userID))):\n",
    "            node1 = candidate_userID[udx]\n",
    "            neighbors = user_to_user_sim[udx]\n",
    "\n",
    "            #devide by the lexicon size\n",
    "\n",
    "            neighbors_decision = np.array(neighbors[(udx+1):])>=link_threshold\n",
    "            neighbors_candidates = np.array(candidate_userID[(udx+1):])[neighbors_decision]\n",
    "            neighbors_weight = np.array(neighbors[(udx+1):])[neighbors_decision]\n",
    "\n",
    "            for ndx in range(0, len(neighbors_candidates)):\n",
    "                neighbor = neighbors_candidates[ndx]\n",
    "                weight = round(neighbors_weight[ndx], 3)\n",
    "                max_weight = max(max_weight, weight)\n",
    "                im_links.append(str(node1)+','+str(neighbor)+','+str(weight))           \n",
    "            #vdx = udx + 1\n",
    "            #while vdx<len(candidate_userID):\n",
    "            #    node2 = candidate_userID[vdx]\n",
    "            #    weight = neighbors[vdx]\n",
    "            #    max_weight = max(max_weight, weight)\n",
    "            #    if weight>=link_threshold:\n",
    "            #         im_links.append(str(node1)+','+str(node2)+','+str(weight))\n",
    "            #    vdx = vdx + 1\n",
    "        return im_links, max_weight\n",
    "\n",
    "\n",
    "def get_explicit_links(retweetUID_tweetUID_tweetID):\n",
    "        ex_links = []\n",
    "        for r_o_t in tqdm(retweetUID_tweetUID_tweetID):\n",
    "            parts = r_o_t.split(',')\n",
    "            node1 = parts[0]\n",
    "            node2 = parts[1]\n",
    "            t = parts[2]   \n",
    "            weight = 1\n",
    "            ex_links.append(str(node1)+','+str(node2)+','+str(weight))\n",
    "        return ex_links\n",
    "\n",
    "\n",
    "def get_node_degree(G):\n",
    "        \"\"\"\n",
    "            returns the nodes with its degree from the graph\n",
    "        \"\"\"\n",
    "        node_sequence = [n for n, d in G.degree()]\n",
    "        degree_sequence = [d for n, d in G.degree()]\n",
    "        max_degree = 0\n",
    "        if len(degree_sequence)>0:\n",
    "            max_degree = max(degree_sequence)\n",
    "        node_degree = dict(zip(node_sequence, degree_sequence))\n",
    "        return node_degree, max_degree\n",
    "\n",
    "def get_nodes_with_degree(G, degree):\n",
    "        return [node for node,degree in dict(G.degree()).items() if degree < 2]\n",
    "\n",
    "\n",
    "print (\"Setting up the data path ...\")\n",
    "data_dir = \"/projets/sig/mullah/nlp/prevision_fgpi\"\n",
    "figures_path = \"../figures/\"\n",
    "graph_dir = data_dir + \"/graph\"\n",
    "lexicon_name = \"Radical\"\n",
    "model = \"BoWs\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#newretweetuserid_neworiginaluserid_path = os.path.join(data_dir, 'data/processed/20000_UserRetweetID_UserOriginalID_NewID.txt')\n",
    "tweetID_msg_path = os.path.join(data_dir, 'data/processed/id_OriginalTweet.json')\n",
    "retweetUserID_tweetUserID_tweetID_path = os.path.join(data_dir, 'data/processed/UserRetweet_UserOriginal_idOriginalTweet.txt')\n",
    "lexicon_path = os.path.join(data_dir, 'lexicons/'+lexicon_name)\n",
    "\n",
    "print (\"Loading the tweetID and messages ...\")\n",
    "tweetID_msg = get_json_content(tweetID_msg_path)\n",
    "print (\"Total tweets: {}\".format(len(tweetID_msg)))\n",
    "\n",
    "print (\"Loading the retweerUser, tweetUser, and tweetId ...\")\n",
    "retweetUserID_tweetUserID_tweetID = get_file_content(retweetUserID_tweetUserID_tweetID_path)\n",
    "print (\"Total retweets: {}\".format(len(retweetUserID_tweetUserID_tweetID)))\n",
    "\n",
    "print (\"Loading the lexicon file ...\")\n",
    "lexicon = get_file_content(lexicon_path)\n",
    "print (\"Total jargons in lexicon: {}\".format(len(lexicon)))\n",
    "\n",
    "print (\"Extracting tweetUser and tweetId ...\")\n",
    "tweetUserID_tweetId = get_tweetUserID_tweetIDs(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total users who tweeted: {}\".format(len(tweetUserID_tweetId)))\n",
    "\n",
    "print (\"Extracting retweetUser and tweetId ...\")\n",
    "retweetUserID_tweetId = get_retweetUserID_tweetIDs(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total users who retweeted: {}\".format(len(retweetUserID_tweetId)))\n",
    "\n",
    "print (\"Getting both tweet and retweet users ...\")\n",
    "bothTweetRetweetUserID = get_common_dict_keys(tweetUserID_tweetId, retweetUserID_tweetId)\n",
    "print (\"Total users who both tweeted and retweeted: {}\".format(len(bothTweetRetweetUserID)))\n",
    "\n",
    "print (\"Getting all users either tweeter or retweeter ...\")\n",
    "userID_tweetIDs = merge_two_dicts(tweetUserID_tweetId, retweetUserID_tweetId)\n",
    "print (\"Total user who tweeted or retweeted: {}\".format(len(userID_tweetIDs)))\n",
    "\n",
    "print (\"Ploting the data ...\")\n",
    "userID_freq, max_freq = get_userID_tweetFreq(userID_tweetIDs)\n",
    "#save_plot_dict_to_histo(userID_freq, \"Distribution of tweet/retweet poster by users\", \"No. of tweets/retweets\", \"No. of users\", \n",
    "#                        ['Max no. of tweets/retweets: '+str(max_freq)], figures_path + \"Users_Tweets_Distribution.png\")\n",
    "save_plot_dict_to_histo(userID_freq, \"Distribution of tweet/retweet posted by users\", \"No. of tweets/retweets\", \"No. of users\", \n",
    "                        None, figures_path + model + '_' +\"Users_Tweets_Distribution.png\")\n",
    "print (\"done.\")\n",
    "\n",
    "print (\"Estimating users lexicon representation ...\")\n",
    "userIDs, lexicon_repr = get_userID_lex_repr(userID_tweetIDs, tweetID_msg, lexicon)\n",
    "print (\"done.\")\n",
    "\n",
    "print (\"Normalizing user's lexicon based represention ...\")\n",
    "lexicon_repr_normalized = get_normalized_representation(lexicon_repr)\n",
    "print (\"done.\")\n",
    "\n",
    "#print (\"Users used jargon from the lexicon...\")\n",
    "#userID_lex_score, max_jargon_used = get_userID_lex_score(userID_lex_repr_normalized)\n",
    "#save_plot_dict_to_histo(userID_lex_score, \"Distribution of jargon in tweets/retweets of users\", \"Numbers of jargon used\", \n",
    "#                        \"No. of users\", ['Max jargon used: '+str(max_jargon_used)], figures_path + \"Users_Radical_Scores_Distribution.png\")\n",
    "\n",
    "#print (\"Maximum jargon used: {}\".format(max_jargon_used))\n",
    "\n",
    "#print (\"Setting up the parameters ...\")\n",
    "#link_weight_threshold = 1\n",
    "\n",
    "print ('Loading explicit links ...')\n",
    "ex_links = get_explicit_links(retweetUserID_tweetUserID_tweetID)\n",
    "print (\"Total explicit links: {}\".format(len(ex_links)))\n",
    "\n",
    "edge_color = 'b'\n",
    "ex_links_df = links_to_df(ex_links, edge_color)\n",
    "G_explicit_link = nx.from_pandas_edgelist(ex_links_df, \"source\", \"target\", create_using=nx.Graph())\n",
    "#print ('Drawing the graph of explicit links only ...')\n",
    "\n",
    "degree = 1\n",
    "nodes_degree = get_nodes_with_degree(G_explicit_link, degree)\n",
    "print (\"Total nodes with degree at most {} is {}\".format(degree, len(nodes_degree)))\n",
    "\n",
    "print (\"Removing nodes ....\")\n",
    "G_explicit_link.remove_nodes_from(nodes_degree)\n",
    "print (\"done.\")\n",
    "\n",
    "#edge_color = 'b'\n",
    "#plots_graph(G_explicit_link, \"Graph of users with explicit links\", edge_color,\n",
    "#            figures_path + \"Graph_of_users_with_explicit_links.png\")\n",
    "#plots_graph(G_explicit_link, \"Graph of users with explicit links\", edge_color,\n",
    "#            figures_path + \"Graph_of_users_with_explicit_links.png\")\n",
    "draw_graph_pyvis(G_explicit_link, figures_path + \"Graph_of_users_with_explicit_links\", \"Graph_of_users_with_explicit_links\")\n",
    "nx.write_gpickle(G_explicit_link, graph_dir + \"/Graph_of_users_with_explicit_links\")\n",
    "\n",
    "#explit_edges = nx.to_edgelist(G_explicit_link)\n",
    "#nodes = nx.nodes\n",
    "\n",
    "print (\"Transforming the candidate users and its representation as data frame\")\n",
    "lexicon_repr_normalized_DF = pd.DataFrame(lexicon_repr_normalized)\n",
    "print (lexicon_repr_normalized_DF.info())\n",
    "    \n",
    "print (\"Computing dot product divide by the length...\")\n",
    "users_Sim = dot_length_similarity(lexicon_repr_normalized_DF, lexicon)\n",
    "print (\"{},{}\".format(len(users_Sim),len(users_Sim[0])))\n",
    "min_sim, max_sim = get_min_max_sim(users_Sim)\n",
    "print (\"{},{}\".format(min_sim, max_sim))\n",
    "\n",
    "#print (\"Computing user similarity ...\")\n",
    "#user_to_user_sim = squareform(pdist(candidate_userID_representation_DF.T, metric='cosine'))\n",
    "#users_Sim = cosine_similarity(lexicon_repr_normalized_DF)\n",
    "#print (\"{},{}\".format(len(users_Sim),len(users_Sim[0])))\n",
    "#print (users_Sim)\n",
    "\n",
    "#for num_jargon in range(1, max_jargon_used):\n",
    "dif = max_sim - min_sim\n",
    "inc = dif/10\n",
    "\n",
    "for step in range(1, 11):\n",
    "#for step in range(5, 6):\n",
    "    #edge_weight = step/10\n",
    "    edge_weight = min_sim + step * inc\n",
    "    print (\"Assessing the candidate users with edge weight {} for step {}\".format(edge_weight, step))\n",
    "\n",
    "    #userID_lex_score_sorted = get_sorted_dict_by_values(userID_lex_score, True)\n",
    "    #candidate_userID = get_candidate_userID(userID_lex_score_sorted, edge_weight)\n",
    "    #print (\"Total candidate users: {}\".format(len(candidate_userID)))\n",
    "\n",
    "    #print (\"Representaiton of candidate users ....\")\n",
    "    #candidate_userID_lex_repr = get_candidate_userID_lex_repr(candidate_userID, userID_lex_repr_normalized)\n",
    "       \n",
    "    print (\"Generating implicit links ...\")\n",
    "    im_links, max_weight = get_implicit_links(userIDs, users_Sim, edge_weight) \n",
    "    print (\"Total implicit links: {} and maximum weight: {}\".format(len(im_links), max_weight))\n",
    "\n",
    "    print (\"Implicit links to data frame ...\")\n",
    "    edge_color = \"r\"\n",
    "    im_links_df = links_to_df(im_links, edge_color)\n",
    "    \n",
    "    print ('writing to file ...')\n",
    "    im_graph_path = graph_dir + '/implicit_' + lexicon_name + '_' + str(step)\n",
    "    im_links_df.to_csv(im_graph_path)\n",
    "    print ('done')\n",
    "\n",
    "    print (\"Creating graph ...\")\n",
    "    G_implicit_link = nx.from_pandas_edgelist(im_links_df, edge_attr=\"edge_weight\", \n",
    "                                              create_using=nx.Graph())\n",
    "    node_degree, max_degree = get_node_degree(G_implicit_link)\n",
    "    if len(node_degree) < 2:\n",
    "        continue\n",
    "    draw_graph_pyvis(G_implicit_link, figures_path + \"Graph_of_users_with_implicit_links_\" + str(step), \"Graph_of_users_with_implicit_links_\" + str(step))\n",
    "    \n",
    "    \"\"\"\n",
    "    print ('Degree distribution of users in the graph of implicit links ...')\n",
    "    save_plot_dict_to_histo(node_degree, \"Degree distribution of users with implicit links\", \"Degree\", \n",
    "                            \"No. of users\", ['Max degree: '+str(max_degree)], figures_path + 'Graph_implicit_degree_'+str(edge_weight)+'.png')\n",
    "\n",
    "    print ('Drawing the graph of implicit links only ...')\n",
    "    plots_graph(G_implicit_link, \"Graph of users with implicit links for edge weight \"+str(edge_weight), \n",
    "                edge_color, figures_path + \"Graph_of_users_with_implicit_links_for_\"+str(edge_weight)+\".png\")\n",
    "\n",
    "    \n",
    "    implicit_edges = nx.to_edgelist(G_implicit_link)\n",
    "    #Gexp_imp = F = nx.compose(G_explicit_link, G_implicit_link)\n",
    "    #print ('Drawing the graph of implicit links only ...')\n",
    "    #plots_graph(G_implicit_link, \"Graph of users with implicit links for \"+str(num_jargon)+\"\", \n",
    "    #            edge_color, figures_path + \"Graph_of_users_with_implicit_links_for_\"+str(num_jargon)+\".png\")\n",
    "    \n",
    "    #print (\"Drawing the superimposed explicit and implicit graphs ... \")\n",
    "    #plots_graph_hetero_edges(nodes, explit_edges, 'b',  implicit_edges, 'r', \n",
    "    #                         \"Graph of users with explicit and implicit links for \"+str(num_jargon)+\"jargon\", \n",
    "    #                        figures_path + \"Graph_of_users_with_explicit_and_implicit_links_for_\"+str(num_jargon)+\"_lexicons.png\")\n",
    "    \"\"\"\n",
    "    #integrating two graphs\n",
    "    extended_graph = nx.Graph()\n",
    "    extended_graph.add_edges_from(G_explicit_link.edges, color=\"blue\")\n",
    "    extended_graph.add_edges_from(G_implicit_link.edges, color=\"red\")\n",
    "    draw_graph_pyvis(extended_graph, figures_path + \"Graph_of_users_with_explicit_and_implicit_links_\" + str(step), \"Graph_of_users_with_explicit_and_implicit_links_\" + str(step))\n",
    "    nx.write_gpickle(extended_graph, graph_dir + \"/Graph_of_users_with_explicit_and_implicit_links_\" + str(step))\n",
    "    \n",
    "print ('done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36 (torch)",
   "language": "python",
   "name": "e36t11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
